{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e222ab8-a5a1-4ae4-8a18-8823b569343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrega 4 Miguel Chaveinte Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3744cc3e-dc77-4835-80b4-88973ab0b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b069191-f923-4126-aaf5-b999a076c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd009e5a-ea15-4033-b85c-dc7104bd231b",
   "metadata": {},
   "source": [
    "# DATOS Y CARGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dcb8433-35d7-49f0-bfb3-32ba1c8ab1d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist=fetch_openml('mnist_784', version=1)\n",
    "mnist.data.shape,mnist.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb63941d-4c42-4ffc-9a08-c32a5598c2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.data.to_csv('mnist_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80253dee-2653-4657-ab45-bc2b312824b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.target.to_csv('mnist_target.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1addcc4-8b92-454f-89b4-3c7968c3a9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(pd.read_csv('./mnist_data.csv'))\n",
    "y=np.array(pd.read_csv('./mnist_target.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25e5e882-936b-41ee-8409-21568347da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1276c76d-a577-44bb-ad7c-78217a55ed9c",
   "metadata": {},
   "source": [
    "# NORMALIZACIÓN EXTENDIDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "81b17ca2-dc03-4025-8673-0cc88ba54a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 785), numpy.ndarray)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_norm=normalize(np.append(X,np.ones((X.shape[0],1),dtype=float).reshape(-1,1),axis=1))\n",
    "X_norm.shape,type(X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f73113-a464-48cd-8e3a-d2f5cdb1891d",
   "metadata": {},
   "source": [
    "### REESCALADO DE LOS DATOS. 600 PARA APRENDIZAJE Y 100 PARA TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec396e26-0a1d-4397-8022-2c7269587949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((700, 785), (700, 1))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XR=X_norm[:700]\n",
    "YR=y[:700]\n",
    "XR.shape,YR.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e85b19-5083-4ef8-a3d5-960154a3bca9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CLASES A UTILIZAR PARA SOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd415029-7189-40f2-9604-ffcae0daa0e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clase Neurona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4db8cd8d-db41-41ff-b937-ab14e4092728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class neurona(object):\n",
    "    def __init__(self, f=0, c=0, dim=0):\n",
    "        self.c = c\n",
    "        self.f = f\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Inicializa los pesos aleatoriamente entre [-0.5, 0.5]\n",
    "        self.w = normalize(0.5 - np.random.rand(dim).reshape(1,-1))\n",
    "        \n",
    "        # self.label_winner = []\n",
    "            \n",
    "    def predict(self, inputs):\n",
    "        # Calcula la salida de una neurona ante una o más entradas. \"inputs\" puede ser un vector o una matriz 2D\n",
    "        return inputs @ self.w.T\n",
    "    \n",
    "    def fit(self, input, alfa=1):\n",
    "        # ajusta los pesos de una neurona (w) para aproximarlos a una entrada (input)\n",
    "        self.w = normalize(self.w + (alfa*input))\n",
    "            \n",
    "    def neuron_labeling(self, inputs, target):\n",
    "        # etiquetado por neuronas. Se le pasa la lista de entradas y la etiqueta (target) de cada una de esas muestras.\n",
    "        # Devuelve la etiqueta de la ganadora\n",
    "        Y = inputs @ self.w.T\n",
    "        self.label = target[np.argmax(Y)]\n",
    "        return self.label\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b6606-6f47-44b8-bfd8-ee2aadbdd287",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clase SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b9a4481-19b0-4083-aff0-6a5ffaa1ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class som():\n",
    "    \n",
    "    def __init__(self, filas=1, columnas=1, dim=1):\n",
    "        self.lista = []\n",
    "        self.filas = filas\n",
    "        self.columnas = columnas\n",
    "        self.dim = dim\n",
    "        self.labels = []\n",
    "        # Considera que un mapa rectangular es una lista de objetos \"neurona\", que viene localizado por sus atributos \"fila\" y \"columna\"\n",
    "        for fila in range(self.filas):\n",
    "            for columna in range(self.columnas):\n",
    "                self.lista.append(neurona(f=fila, c=columna, dim=dim))\n",
    "                \n",
    "    def fit(self, inputs, max_epochs=1, init_radious=0, init_alfa=1):\n",
    "        # método similar a otros algoritmo de ML. Recibe las entradas, el radio inicial, el factor de apendizaje inicial,\n",
    "        # el máximo de épocas y devuelve los pesos ajustados\n",
    "        self.radious = init_radious\n",
    "        self.alfa = init_alfa\n",
    "        t = 0\n",
    "        P = inputs.shape[0]\n",
    "        for epoch in range(max_epochs):\n",
    "            for x in inputs:\n",
    "                self.alfa = init_alfa/(1.0 + float(t/P))\n",
    "                i_gana, y_gana = -1, float('-inf')\n",
    "                for i in range(self.filas*self.columnas):\n",
    "                    y_predict = self.lista[i].predict(x.reshape(1,-1))\n",
    "                    if y_predict > y_gana:\n",
    "                        y_gana = y_predict\n",
    "                        i_gana = i\n",
    "                f_gana = int(i_gana / self.columnas)\n",
    "                c_gana = i_gana % self.columnas\n",
    "                \n",
    "                # Conjunto de vecinas para un radious\n",
    "                for f in range(f_gana - self.radious, f_gana + self.radious+1):\n",
    "                    if f < 0:\n",
    "                        row = self.filas + f\n",
    "                    else:\n",
    "                        if f > self.filas-1:\n",
    "                            row = f % self.filas\n",
    "                        else:\n",
    "                            row = f\n",
    "\n",
    "                    for c in range(c_gana - self.radious, c_gana + self.radious+1):\n",
    "                        if c < 0:\n",
    "                            column = self.columnas + c \n",
    "                        else:\n",
    "                            if c > self.columnas-1:\n",
    "                                column = c % self.columnas\n",
    "                            else:\n",
    "                                column = c\n",
    "                        self.lista[(row*self.columnas) + column].fit(x.reshape(1,-1), alfa=self.alfa)\n",
    "                t += 1\n",
    "                if (t%1000) == 0:\n",
    "                    print(t, self.radious, \"  \", end='')\n",
    "            if self.radious > 0:\n",
    "                self.radious -= 1\n",
    "                            \n",
    "    def neuron_labeling(self, inputs, target):\n",
    "        # recorre la lista de neuronas y va llamanado a su metodo de etiquetado para cada neurona\n",
    "        \n",
    "        for i in range(self.filas*self.columnas):\n",
    "            # print(X.shape, self.target.shape)\n",
    "            self.labels.append(self.lista[i].neuron_labeling(inputs, target))\n",
    "            # print(self.lista[i].labeling(X, target=y_deseada, etiquetado='neurona'))\n",
    " \n",
    "    def predict(self, inputs):\n",
    "        # recorre la lista de neuronas y calcula la salida de un conjunto de muestras\n",
    "        # util para usar la salida del som como entrada a otrso sistemas\n",
    "        output_list = []\n",
    "        for x in inputs:\n",
    "            for i in range(self.filas*self.columnas):\n",
    "                output_list.append(self.lista[i].predict(x.reshape(1,-1)))\n",
    "        return np.array(output_list).reshape(inputs.shape[0], -1)    \n",
    "    \n",
    "    def label_predict(self, inputs):\n",
    "        # clasificación de muestras con el etiquetado de cada neurona hecho previamente\n",
    "        label_list = []\n",
    "        for x in inputs:\n",
    "            label_list.append(self.labels[np.argmax(self.predict(x.reshape(1,-1)))])\n",
    "        return np.array(label_list).reshape(inputs.shape[0], -1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61143a-975b-43ad-a608-dbf5457cf6ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaf5bed5-bd82-45b0-85e0-96da559d5757",
   "metadata": {},
   "source": [
    "# DIMENSIÓN DEL SOM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36a3562e-0190-453c-8513-5ff3f61cc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filas=15\n",
    "columnas=9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "516fb340-9a5b-45c9-86a1-db056ce81950",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(XR,YR,test_size=100,stratify=YR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84dc296f-cacd-4f3b-8e44-ba044e6b053e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((600, 785), (100, 785), (600, 1), (100, 1))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ef4c3368-e179-436b-9a82-1655275024fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_autorganizado=som(filas=15,columnas=9,dim=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58518c6b-d9ec-471d-bdc4-aab6096121c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 3   2000 1   3000 0   4000 0   5000 0   6000 0   7000 0   8000 0   9000 0   10000 0   11000 0   12000 0   13000 0   14000 0   15000 0   16000 0   17000 0   18000 0   "
     ]
    }
   ],
   "source": [
    "mapa_autorganizado.fit(inputs=X_train,max_epochs=30,init_radious=4,init_alfa=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "646d0277-8f33-42ab-8b98-8ecfebd9a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa_autorganizado.neuron_labeling(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34e89f69-dac7-4401-ac96-1d46775146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "etiquetas_neuronas_predict=mapa_autorganizado.label_predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88c36a8b-c035-447e-8719-195cd9d7993b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,etiquetas_neuronas_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0645db0-0dbf-4e68-8aca-aa5111268684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fda8e70b-c96a-46c9-89bf-d575a3883f52",
   "metadata": {},
   "source": [
    "# MLP (135X60X10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd46ebda-a687-4e71-af9e-1135abdad21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07b2e35b-919d-4b83-b657-1d1d9ee9ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp=mapa_autorganizado.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e919ca96-c902-40c2-a5da-73ca73729edb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 135)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c9427dd-1981-48d4-9f2d-e27f698df89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp=X_train_mlp.reshape(600,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf4358bc-1836-4d58-84d5-9e7ef8bb72d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 135)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5735f43-6973-4dde-94a4-82d037a6f220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17a84a81-29fe-4bc7-9ff7-eea5d0f28c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mlp=mapa_autorganizado.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a89c903e-2a35-47f6-9014-131e27e85250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 135)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2927ef18-5a53-498e-b6a0-51c3d5c0f5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mlp=X_test_mlp.reshape(100,135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5207d3cd-359a-49c7-b9ad-6325bf30a93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 135)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_mlp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0654a401-384b-46f9-9c0e-d3626d9c1783",
   "metadata": {},
   "source": [
    "### ESCALADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "41e6d618-6b1e-45e0-b353-02d0b519eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "escalado=MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c6d29b8-4f6d-4434-b7cd-6667305d444e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escalado.fit(np.vstack((X_train_mlp,X_test_mlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36e805b9-6d50-47c0-99bf-a507c9baa4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mlp=escalado.transform(X_train_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b912f3c2-d80d-4fb0-b331-ffbd74e4641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_mlp=escalado.transform(X_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d7a46bc4-ffa9-4384-9e78-c14b340ee309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e44d6c48-371d-48f6-9e2d-5b8b01b01670",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp=MLPClassifier(hidden_layer_sizes=(60,),verbose=True,max_iter=3000) #verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "01fe6ee2-0692-4ddb-8e1e-4e47305c293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\migue\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1109: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.42379771\n",
      "Iteration 2, loss = 2.29450130\n",
      "Iteration 3, loss = 2.21478745\n",
      "Iteration 4, loss = 2.17160388\n",
      "Iteration 5, loss = 2.14099737\n",
      "Iteration 6, loss = 2.10523645\n",
      "Iteration 7, loss = 2.06553810\n",
      "Iteration 8, loss = 2.02537418\n",
      "Iteration 9, loss = 1.98662336\n",
      "Iteration 10, loss = 1.94841878\n",
      "Iteration 11, loss = 1.91059026\n",
      "Iteration 12, loss = 1.87206474\n",
      "Iteration 13, loss = 1.83219263\n",
      "Iteration 14, loss = 1.79328852\n",
      "Iteration 15, loss = 1.75574516\n",
      "Iteration 16, loss = 1.71862040\n",
      "Iteration 17, loss = 1.68258912\n",
      "Iteration 18, loss = 1.64722065\n",
      "Iteration 19, loss = 1.61054660\n",
      "Iteration 20, loss = 1.57680770\n",
      "Iteration 21, loss = 1.54218633\n",
      "Iteration 22, loss = 1.50847363\n",
      "Iteration 23, loss = 1.47729794\n",
      "Iteration 24, loss = 1.44403752\n",
      "Iteration 25, loss = 1.41211590\n",
      "Iteration 26, loss = 1.38125933\n",
      "Iteration 27, loss = 1.35006750\n",
      "Iteration 28, loss = 1.32173502\n",
      "Iteration 29, loss = 1.29254615\n",
      "Iteration 30, loss = 1.26439541\n",
      "Iteration 31, loss = 1.23797193\n",
      "Iteration 32, loss = 1.21161952\n",
      "Iteration 33, loss = 1.18651692\n",
      "Iteration 34, loss = 1.16289173\n",
      "Iteration 35, loss = 1.13920392\n",
      "Iteration 36, loss = 1.11632392\n",
      "Iteration 37, loss = 1.09488462\n",
      "Iteration 38, loss = 1.07355882\n",
      "Iteration 39, loss = 1.05336877\n",
      "Iteration 40, loss = 1.03374953\n",
      "Iteration 41, loss = 1.01544996\n",
      "Iteration 42, loss = 0.99665318\n",
      "Iteration 43, loss = 0.97956255\n",
      "Iteration 44, loss = 0.96229395\n",
      "Iteration 45, loss = 0.94632636\n",
      "Iteration 46, loss = 0.93058739\n",
      "Iteration 47, loss = 0.91486422\n",
      "Iteration 48, loss = 0.90077255\n",
      "Iteration 49, loss = 0.88663938\n",
      "Iteration 50, loss = 0.87315018\n",
      "Iteration 51, loss = 0.85957806\n",
      "Iteration 52, loss = 0.84676598\n",
      "Iteration 53, loss = 0.83477001\n",
      "Iteration 54, loss = 0.82225880\n",
      "Iteration 55, loss = 0.81284051\n",
      "Iteration 56, loss = 0.80006625\n",
      "Iteration 57, loss = 0.78895526\n",
      "Iteration 58, loss = 0.77856802\n",
      "Iteration 59, loss = 0.76900000\n",
      "Iteration 60, loss = 0.75881167\n",
      "Iteration 61, loss = 0.74916419\n",
      "Iteration 62, loss = 0.74031313\n",
      "Iteration 63, loss = 0.73143787\n",
      "Iteration 64, loss = 0.72213438\n",
      "Iteration 65, loss = 0.71414005\n",
      "Iteration 66, loss = 0.70627059\n",
      "Iteration 67, loss = 0.69753636\n",
      "Iteration 68, loss = 0.69007823\n",
      "Iteration 69, loss = 0.68270357\n",
      "Iteration 70, loss = 0.67548191\n",
      "Iteration 71, loss = 0.66880555\n",
      "Iteration 72, loss = 0.66261031\n",
      "Iteration 73, loss = 0.65504902\n",
      "Iteration 74, loss = 0.64855269\n",
      "Iteration 75, loss = 0.64286827\n",
      "Iteration 76, loss = 0.63682352\n",
      "Iteration 77, loss = 0.63055181\n",
      "Iteration 78, loss = 0.62540721\n",
      "Iteration 79, loss = 0.61855860\n",
      "Iteration 80, loss = 0.61268934\n",
      "Iteration 81, loss = 0.60786237\n",
      "Iteration 82, loss = 0.60355331\n",
      "Iteration 83, loss = 0.59733888\n",
      "Iteration 84, loss = 0.59207086\n",
      "Iteration 85, loss = 0.58682971\n",
      "Iteration 86, loss = 0.58218633\n",
      "Iteration 87, loss = 0.57760177\n",
      "Iteration 88, loss = 0.57268056\n",
      "Iteration 89, loss = 0.56794341\n",
      "Iteration 90, loss = 0.56309419\n",
      "Iteration 91, loss = 0.55964435\n",
      "Iteration 92, loss = 0.55449451\n",
      "Iteration 93, loss = 0.55018355\n",
      "Iteration 94, loss = 0.54667588\n",
      "Iteration 95, loss = 0.54262375\n",
      "Iteration 96, loss = 0.53922492\n",
      "Iteration 97, loss = 0.53533971\n",
      "Iteration 98, loss = 0.53118036\n",
      "Iteration 99, loss = 0.52734753\n",
      "Iteration 100, loss = 0.52473530\n",
      "Iteration 101, loss = 0.51981094\n",
      "Iteration 102, loss = 0.51643731\n",
      "Iteration 103, loss = 0.51309203\n",
      "Iteration 104, loss = 0.50990701\n",
      "Iteration 105, loss = 0.50593465\n",
      "Iteration 106, loss = 0.50341488\n",
      "Iteration 107, loss = 0.49917304\n",
      "Iteration 108, loss = 0.49606707\n",
      "Iteration 109, loss = 0.49295715\n",
      "Iteration 110, loss = 0.48926187\n",
      "Iteration 111, loss = 0.48644071\n",
      "Iteration 112, loss = 0.48347698\n",
      "Iteration 113, loss = 0.48051159\n",
      "Iteration 114, loss = 0.47749391\n",
      "Iteration 115, loss = 0.47444462\n",
      "Iteration 116, loss = 0.47205025\n",
      "Iteration 117, loss = 0.46904228\n",
      "Iteration 118, loss = 0.46671367\n",
      "Iteration 119, loss = 0.46342277\n",
      "Iteration 120, loss = 0.46084009\n",
      "Iteration 121, loss = 0.45919657\n",
      "Iteration 122, loss = 0.45618041\n",
      "Iteration 123, loss = 0.45310745\n",
      "Iteration 124, loss = 0.45082458\n",
      "Iteration 125, loss = 0.44826049\n",
      "Iteration 126, loss = 0.44549218\n",
      "Iteration 127, loss = 0.44301644\n",
      "Iteration 128, loss = 0.44134013\n",
      "Iteration 129, loss = 0.43810425\n",
      "Iteration 130, loss = 0.43624717\n",
      "Iteration 131, loss = 0.43499752\n",
      "Iteration 132, loss = 0.43278378\n",
      "Iteration 133, loss = 0.42895029\n",
      "Iteration 134, loss = 0.42631527\n",
      "Iteration 135, loss = 0.42495267\n",
      "Iteration 136, loss = 0.42287410\n",
      "Iteration 137, loss = 0.42025988\n",
      "Iteration 138, loss = 0.41895243\n",
      "Iteration 139, loss = 0.41529751\n",
      "Iteration 140, loss = 0.41352527\n",
      "Iteration 141, loss = 0.41166518\n",
      "Iteration 142, loss = 0.40929997\n",
      "Iteration 143, loss = 0.40718646\n",
      "Iteration 144, loss = 0.40715792\n",
      "Iteration 145, loss = 0.40378827\n",
      "Iteration 146, loss = 0.40154406\n",
      "Iteration 147, loss = 0.39949142\n",
      "Iteration 148, loss = 0.39778957\n",
      "Iteration 149, loss = 0.39634228\n",
      "Iteration 150, loss = 0.39404869\n",
      "Iteration 151, loss = 0.39176094\n",
      "Iteration 152, loss = 0.39031223\n",
      "Iteration 153, loss = 0.38850891\n",
      "Iteration 154, loss = 0.38683342\n",
      "Iteration 155, loss = 0.38473889\n",
      "Iteration 156, loss = 0.38264110\n",
      "Iteration 157, loss = 0.38141340\n",
      "Iteration 158, loss = 0.37953950\n",
      "Iteration 159, loss = 0.37753546\n",
      "Iteration 160, loss = 0.37667880\n",
      "Iteration 161, loss = 0.37460864\n",
      "Iteration 162, loss = 0.37313033\n",
      "Iteration 163, loss = 0.37172455\n",
      "Iteration 164, loss = 0.36977620\n",
      "Iteration 165, loss = 0.36770070\n",
      "Iteration 166, loss = 0.36688619\n",
      "Iteration 167, loss = 0.36485672\n",
      "Iteration 168, loss = 0.36308530\n",
      "Iteration 169, loss = 0.36189723\n",
      "Iteration 170, loss = 0.36084701\n",
      "Iteration 171, loss = 0.35906061\n",
      "Iteration 172, loss = 0.35721356\n",
      "Iteration 173, loss = 0.35558671\n",
      "Iteration 174, loss = 0.35402144\n",
      "Iteration 175, loss = 0.35256484\n",
      "Iteration 176, loss = 0.35143608\n",
      "Iteration 177, loss = 0.35026352\n",
      "Iteration 178, loss = 0.34756844\n",
      "Iteration 179, loss = 0.34805095\n",
      "Iteration 180, loss = 0.34528934\n",
      "Iteration 181, loss = 0.34385335\n",
      "Iteration 182, loss = 0.34312596\n",
      "Iteration 183, loss = 0.34235123\n",
      "Iteration 184, loss = 0.34002738\n",
      "Iteration 185, loss = 0.33926156\n",
      "Iteration 186, loss = 0.33761045\n",
      "Iteration 187, loss = 0.33639560\n",
      "Iteration 188, loss = 0.33445051\n",
      "Iteration 189, loss = 0.33333317\n",
      "Iteration 190, loss = 0.33155858\n",
      "Iteration 191, loss = 0.33112985\n",
      "Iteration 192, loss = 0.32918442\n",
      "Iteration 193, loss = 0.32760122\n",
      "Iteration 194, loss = 0.32719559\n",
      "Iteration 195, loss = 0.32559446\n",
      "Iteration 196, loss = 0.32466993\n",
      "Iteration 197, loss = 0.32283443\n",
      "Iteration 198, loss = 0.32290482\n",
      "Iteration 199, loss = 0.32115349\n",
      "Iteration 200, loss = 0.31923602\n",
      "Iteration 201, loss = 0.31820511\n",
      "Iteration 202, loss = 0.31743178\n",
      "Iteration 203, loss = 0.31663142\n",
      "Iteration 204, loss = 0.31552593\n",
      "Iteration 205, loss = 0.31352345\n",
      "Iteration 206, loss = 0.31252760\n",
      "Iteration 207, loss = 0.31177192\n",
      "Iteration 208, loss = 0.31029818\n",
      "Iteration 209, loss = 0.30963090\n",
      "Iteration 210, loss = 0.30804735\n",
      "Iteration 211, loss = 0.30640434\n",
      "Iteration 212, loss = 0.30521632\n",
      "Iteration 213, loss = 0.30426535\n",
      "Iteration 214, loss = 0.30371449\n",
      "Iteration 215, loss = 0.30209577\n",
      "Iteration 216, loss = 0.30193130\n",
      "Iteration 217, loss = 0.30058755\n",
      "Iteration 218, loss = 0.29893726\n",
      "Iteration 219, loss = 0.29840598\n",
      "Iteration 220, loss = 0.29717919\n",
      "Iteration 221, loss = 0.29582564\n",
      "Iteration 222, loss = 0.29564528\n",
      "Iteration 223, loss = 0.29382690\n",
      "Iteration 224, loss = 0.29302807\n",
      "Iteration 225, loss = 0.29167487\n",
      "Iteration 226, loss = 0.29070192\n",
      "Iteration 227, loss = 0.29079751\n",
      "Iteration 228, loss = 0.29017277\n",
      "Iteration 229, loss = 0.28838040\n",
      "Iteration 230, loss = 0.28700396\n",
      "Iteration 231, loss = 0.28639235\n",
      "Iteration 232, loss = 0.28577261\n",
      "Iteration 233, loss = 0.28414339\n",
      "Iteration 234, loss = 0.28330561\n",
      "Iteration 235, loss = 0.28195563\n",
      "Iteration 236, loss = 0.28155154\n",
      "Iteration 237, loss = 0.28031734\n",
      "Iteration 238, loss = 0.27940551\n",
      "Iteration 239, loss = 0.27841329\n",
      "Iteration 240, loss = 0.27787556\n",
      "Iteration 241, loss = 0.27697777\n",
      "Iteration 242, loss = 0.27566785\n",
      "Iteration 243, loss = 0.27567625\n",
      "Iteration 244, loss = 0.27426685\n",
      "Iteration 245, loss = 0.27295209\n",
      "Iteration 246, loss = 0.27300064\n",
      "Iteration 247, loss = 0.27123976\n",
      "Iteration 248, loss = 0.27035487\n",
      "Iteration 249, loss = 0.26941064\n",
      "Iteration 250, loss = 0.26824657\n",
      "Iteration 251, loss = 0.26812292\n",
      "Iteration 252, loss = 0.26686194\n",
      "Iteration 253, loss = 0.26624163\n",
      "Iteration 254, loss = 0.26514619\n",
      "Iteration 255, loss = 0.26483643\n",
      "Iteration 256, loss = 0.26454006\n",
      "Iteration 257, loss = 0.26263836\n",
      "Iteration 258, loss = 0.26222748\n",
      "Iteration 259, loss = 0.26095736\n",
      "Iteration 260, loss = 0.25966936\n",
      "Iteration 261, loss = 0.25905988\n",
      "Iteration 262, loss = 0.25818038\n",
      "Iteration 263, loss = 0.25697065\n",
      "Iteration 264, loss = 0.25670043\n",
      "Iteration 265, loss = 0.25627504\n",
      "Iteration 266, loss = 0.25573650\n",
      "Iteration 267, loss = 0.25462119\n",
      "Iteration 268, loss = 0.25397918\n",
      "Iteration 269, loss = 0.25271228\n",
      "Iteration 270, loss = 0.25210226\n",
      "Iteration 271, loss = 0.25122550\n",
      "Iteration 272, loss = 0.25097809\n",
      "Iteration 273, loss = 0.24983879\n",
      "Iteration 274, loss = 0.24977865\n",
      "Iteration 275, loss = 0.24847121\n",
      "Iteration 276, loss = 0.24684458\n",
      "Iteration 277, loss = 0.24761066\n",
      "Iteration 278, loss = 0.24618637\n",
      "Iteration 279, loss = 0.24504818\n",
      "Iteration 280, loss = 0.24459834\n",
      "Iteration 281, loss = 0.24442175\n",
      "Iteration 282, loss = 0.24360077\n",
      "Iteration 283, loss = 0.24271102\n",
      "Iteration 284, loss = 0.24186254\n",
      "Iteration 285, loss = 0.24101293\n",
      "Iteration 286, loss = 0.24028556\n",
      "Iteration 287, loss = 0.23950346\n",
      "Iteration 288, loss = 0.23882602\n",
      "Iteration 289, loss = 0.23809951\n",
      "Iteration 290, loss = 0.23775497\n",
      "Iteration 291, loss = 0.23724357\n",
      "Iteration 292, loss = 0.23565724\n",
      "Iteration 293, loss = 0.23503273\n",
      "Iteration 294, loss = 0.23415156\n",
      "Iteration 295, loss = 0.23439246\n",
      "Iteration 296, loss = 0.23268540\n",
      "Iteration 297, loss = 0.23263169\n",
      "Iteration 298, loss = 0.23153815\n",
      "Iteration 299, loss = 0.23096456\n",
      "Iteration 300, loss = 0.23050028\n",
      "Iteration 301, loss = 0.22935164\n",
      "Iteration 302, loss = 0.22910394\n",
      "Iteration 303, loss = 0.22803090\n",
      "Iteration 304, loss = 0.22776274\n",
      "Iteration 305, loss = 0.22741165\n",
      "Iteration 306, loss = 0.22593435\n",
      "Iteration 307, loss = 0.22545797\n",
      "Iteration 308, loss = 0.22480235\n",
      "Iteration 309, loss = 0.22510864\n",
      "Iteration 310, loss = 0.22388944\n",
      "Iteration 311, loss = 0.22324727\n",
      "Iteration 312, loss = 0.22262464\n",
      "Iteration 313, loss = 0.22193556\n",
      "Iteration 314, loss = 0.22063380\n",
      "Iteration 315, loss = 0.22021663\n",
      "Iteration 316, loss = 0.22010161\n",
      "Iteration 317, loss = 0.22003917\n",
      "Iteration 318, loss = 0.21792493\n",
      "Iteration 319, loss = 0.21787795\n",
      "Iteration 320, loss = 0.21716731\n",
      "Iteration 321, loss = 0.21651571\n",
      "Iteration 322, loss = 0.21585617\n",
      "Iteration 323, loss = 0.21566037\n",
      "Iteration 324, loss = 0.21497618\n",
      "Iteration 325, loss = 0.21473519\n",
      "Iteration 326, loss = 0.21472493\n",
      "Iteration 327, loss = 0.21302613\n",
      "Iteration 328, loss = 0.21253282\n",
      "Iteration 329, loss = 0.21184031\n",
      "Iteration 330, loss = 0.21080797\n",
      "Iteration 331, loss = 0.21067107\n",
      "Iteration 332, loss = 0.20948690\n",
      "Iteration 333, loss = 0.20865750\n",
      "Iteration 334, loss = 0.20847219\n",
      "Iteration 335, loss = 0.20782287\n",
      "Iteration 336, loss = 0.20716366\n",
      "Iteration 337, loss = 0.20699417\n",
      "Iteration 338, loss = 0.20604103\n",
      "Iteration 339, loss = 0.20517601\n",
      "Iteration 340, loss = 0.20475366\n",
      "Iteration 341, loss = 0.20461498\n",
      "Iteration 342, loss = 0.20446104\n",
      "Iteration 343, loss = 0.20384601\n",
      "Iteration 344, loss = 0.20264150\n",
      "Iteration 345, loss = 0.20278716\n",
      "Iteration 346, loss = 0.20227303\n",
      "Iteration 347, loss = 0.20128640\n",
      "Iteration 348, loss = 0.20035278\n",
      "Iteration 349, loss = 0.20032236\n",
      "Iteration 350, loss = 0.20009137\n",
      "Iteration 351, loss = 0.19880838\n",
      "Iteration 352, loss = 0.19846125\n",
      "Iteration 353, loss = 0.19806165\n",
      "Iteration 354, loss = 0.19700641\n",
      "Iteration 355, loss = 0.19657540\n",
      "Iteration 356, loss = 0.19604679\n",
      "Iteration 357, loss = 0.19663499\n",
      "Iteration 358, loss = 0.19508841\n",
      "Iteration 359, loss = 0.19446652\n",
      "Iteration 360, loss = 0.19365758\n",
      "Iteration 361, loss = 0.19313578\n",
      "Iteration 362, loss = 0.19338488\n",
      "Iteration 363, loss = 0.19212145\n",
      "Iteration 364, loss = 0.19157608\n",
      "Iteration 365, loss = 0.19120131\n",
      "Iteration 366, loss = 0.19135184\n",
      "Iteration 367, loss = 0.19104439\n",
      "Iteration 368, loss = 0.18914222\n",
      "Iteration 369, loss = 0.18870930\n",
      "Iteration 370, loss = 0.18930952\n",
      "Iteration 371, loss = 0.18832887\n",
      "Iteration 372, loss = 0.18747585\n",
      "Iteration 373, loss = 0.18701506\n",
      "Iteration 374, loss = 0.18691574\n",
      "Iteration 375, loss = 0.18618266\n",
      "Iteration 376, loss = 0.18531336\n",
      "Iteration 377, loss = 0.18532128\n",
      "Iteration 378, loss = 0.18488040\n",
      "Iteration 379, loss = 0.18380010\n",
      "Iteration 380, loss = 0.18351238\n",
      "Iteration 381, loss = 0.18344153\n",
      "Iteration 382, loss = 0.18253894\n",
      "Iteration 383, loss = 0.18260559\n",
      "Iteration 384, loss = 0.18193858\n",
      "Iteration 385, loss = 0.18113586\n",
      "Iteration 386, loss = 0.18017300\n",
      "Iteration 387, loss = 0.17997223\n",
      "Iteration 388, loss = 0.18012408\n",
      "Iteration 389, loss = 0.17922294\n",
      "Iteration 390, loss = 0.17889761\n",
      "Iteration 391, loss = 0.17791800\n",
      "Iteration 392, loss = 0.17815984\n",
      "Iteration 393, loss = 0.17753525\n",
      "Iteration 394, loss = 0.17644788\n",
      "Iteration 395, loss = 0.17654670\n",
      "Iteration 396, loss = 0.17594224\n",
      "Iteration 397, loss = 0.17535776\n",
      "Iteration 398, loss = 0.17486328\n",
      "Iteration 399, loss = 0.17394748\n",
      "Iteration 400, loss = 0.17409335\n",
      "Iteration 401, loss = 0.17431836\n",
      "Iteration 402, loss = 0.17234474\n",
      "Iteration 403, loss = 0.17255324\n",
      "Iteration 404, loss = 0.17256747\n",
      "Iteration 405, loss = 0.17180272\n",
      "Iteration 406, loss = 0.17082312\n",
      "Iteration 407, loss = 0.17038767\n",
      "Iteration 408, loss = 0.17076118\n",
      "Iteration 409, loss = 0.16964247\n",
      "Iteration 410, loss = 0.16942721\n",
      "Iteration 411, loss = 0.16937016\n",
      "Iteration 412, loss = 0.16846625\n",
      "Iteration 413, loss = 0.16778460\n",
      "Iteration 414, loss = 0.16714750\n",
      "Iteration 415, loss = 0.16767925\n",
      "Iteration 416, loss = 0.16674492\n",
      "Iteration 417, loss = 0.16633647\n",
      "Iteration 418, loss = 0.16686007\n",
      "Iteration 419, loss = 0.16561502\n",
      "Iteration 420, loss = 0.16470190\n",
      "Iteration 421, loss = 0.16426482\n",
      "Iteration 422, loss = 0.16419163\n",
      "Iteration 423, loss = 0.16336503\n",
      "Iteration 424, loss = 0.16353818\n",
      "Iteration 425, loss = 0.16286935\n",
      "Iteration 426, loss = 0.16196211\n",
      "Iteration 427, loss = 0.16167457\n",
      "Iteration 428, loss = 0.16146703\n",
      "Iteration 429, loss = 0.16069936\n",
      "Iteration 430, loss = 0.16032850\n",
      "Iteration 431, loss = 0.15992916\n",
      "Iteration 432, loss = 0.15968047\n",
      "Iteration 433, loss = 0.15881235\n",
      "Iteration 434, loss = 0.15893510\n",
      "Iteration 435, loss = 0.15854802\n",
      "Iteration 436, loss = 0.15738372\n",
      "Iteration 437, loss = 0.15780626\n",
      "Iteration 438, loss = 0.15762148\n",
      "Iteration 439, loss = 0.15664499\n",
      "Iteration 440, loss = 0.15594777\n",
      "Iteration 441, loss = 0.15589250\n",
      "Iteration 442, loss = 0.15510933\n",
      "Iteration 443, loss = 0.15508269\n",
      "Iteration 444, loss = 0.15430863\n",
      "Iteration 445, loss = 0.15429853\n",
      "Iteration 446, loss = 0.15431725\n",
      "Iteration 447, loss = 0.15371574\n",
      "Iteration 448, loss = 0.15315253\n",
      "Iteration 449, loss = 0.15341651\n",
      "Iteration 450, loss = 0.15188916\n",
      "Iteration 451, loss = 0.15153599\n",
      "Iteration 452, loss = 0.15205534\n",
      "Iteration 453, loss = 0.15172696\n",
      "Iteration 454, loss = 0.15056098\n",
      "Iteration 455, loss = 0.15021085\n",
      "Iteration 456, loss = 0.14950904\n",
      "Iteration 457, loss = 0.14936537\n",
      "Iteration 458, loss = 0.14888099\n",
      "Iteration 459, loss = 0.14826419\n",
      "Iteration 460, loss = 0.14784715\n",
      "Iteration 461, loss = 0.14809985\n",
      "Iteration 462, loss = 0.14715719\n",
      "Iteration 463, loss = 0.14701205\n",
      "Iteration 464, loss = 0.14647491\n",
      "Iteration 465, loss = 0.14640041\n",
      "Iteration 466, loss = 0.14548478\n",
      "Iteration 467, loss = 0.14505227\n",
      "Iteration 468, loss = 0.14536259\n",
      "Iteration 469, loss = 0.14475399\n",
      "Iteration 470, loss = 0.14371756\n",
      "Iteration 471, loss = 0.14423013\n",
      "Iteration 472, loss = 0.14363145\n",
      "Iteration 473, loss = 0.14278263\n",
      "Iteration 474, loss = 0.14293735\n",
      "Iteration 475, loss = 0.14203564\n",
      "Iteration 476, loss = 0.14187687\n",
      "Iteration 477, loss = 0.14117710\n",
      "Iteration 478, loss = 0.14118566\n",
      "Iteration 479, loss = 0.14140708\n",
      "Iteration 480, loss = 0.14019304\n",
      "Iteration 481, loss = 0.14023601\n",
      "Iteration 482, loss = 0.13956653\n",
      "Iteration 483, loss = 0.13903167\n",
      "Iteration 484, loss = 0.13961370\n",
      "Iteration 485, loss = 0.13879671\n",
      "Iteration 486, loss = 0.13843930\n",
      "Iteration 487, loss = 0.13856161\n",
      "Iteration 488, loss = 0.13735834\n",
      "Iteration 489, loss = 0.13698438\n",
      "Iteration 490, loss = 0.13698603\n",
      "Iteration 491, loss = 0.13634897\n",
      "Iteration 492, loss = 0.13570745\n",
      "Iteration 493, loss = 0.13693813\n",
      "Iteration 494, loss = 0.13510970\n",
      "Iteration 495, loss = 0.13459958\n",
      "Iteration 496, loss = 0.13530239\n",
      "Iteration 497, loss = 0.13490230\n",
      "Iteration 498, loss = 0.13408708\n",
      "Iteration 499, loss = 0.13354267\n",
      "Iteration 500, loss = 0.13338152\n",
      "Iteration 501, loss = 0.13280993\n",
      "Iteration 502, loss = 0.13236459\n",
      "Iteration 503, loss = 0.13239344\n",
      "Iteration 504, loss = 0.13164499\n",
      "Iteration 505, loss = 0.13132883\n",
      "Iteration 506, loss = 0.13125575\n",
      "Iteration 507, loss = 0.13110616\n",
      "Iteration 508, loss = 0.13029005\n",
      "Iteration 509, loss = 0.12996943\n",
      "Iteration 510, loss = 0.12976039\n",
      "Iteration 511, loss = 0.12921729\n",
      "Iteration 512, loss = 0.12954444\n",
      "Iteration 513, loss = 0.12905734\n",
      "Iteration 514, loss = 0.12872820\n",
      "Iteration 515, loss = 0.12806689\n",
      "Iteration 516, loss = 0.12868127\n",
      "Iteration 517, loss = 0.12747406\n",
      "Iteration 518, loss = 0.12672975\n",
      "Iteration 519, loss = 0.12653748\n",
      "Iteration 520, loss = 0.12661576\n",
      "Iteration 521, loss = 0.12653152\n",
      "Iteration 522, loss = 0.12584714\n",
      "Iteration 523, loss = 0.12618707\n",
      "Iteration 524, loss = 0.12559900\n",
      "Iteration 525, loss = 0.12483041\n",
      "Iteration 526, loss = 0.12451298\n",
      "Iteration 527, loss = 0.12446375\n",
      "Iteration 528, loss = 0.12350195\n",
      "Iteration 529, loss = 0.12355944\n",
      "Iteration 530, loss = 0.12341019\n",
      "Iteration 531, loss = 0.12297500\n",
      "Iteration 532, loss = 0.12242088\n",
      "Iteration 533, loss = 0.12232886\n",
      "Iteration 534, loss = 0.12180591\n",
      "Iteration 535, loss = 0.12137416\n",
      "Iteration 536, loss = 0.12170288\n",
      "Iteration 537, loss = 0.12107525\n",
      "Iteration 538, loss = 0.12082335\n",
      "Iteration 539, loss = 0.12052564\n",
      "Iteration 540, loss = 0.12012464\n",
      "Iteration 541, loss = 0.12080562\n",
      "Iteration 542, loss = 0.11931995\n",
      "Iteration 543, loss = 0.11874762\n",
      "Iteration 544, loss = 0.11885210\n",
      "Iteration 545, loss = 0.11845541\n",
      "Iteration 546, loss = 0.11840806\n",
      "Iteration 547, loss = 0.11862265\n",
      "Iteration 548, loss = 0.11758914\n",
      "Iteration 549, loss = 0.11821276\n",
      "Iteration 550, loss = 0.11852000\n",
      "Iteration 551, loss = 0.11733261\n",
      "Iteration 552, loss = 0.11653250\n",
      "Iteration 553, loss = 0.11606696\n",
      "Iteration 554, loss = 0.11723713\n",
      "Iteration 555, loss = 0.11604940\n",
      "Iteration 556, loss = 0.11552204\n",
      "Iteration 557, loss = 0.11504645\n",
      "Iteration 558, loss = 0.11429739\n",
      "Iteration 559, loss = 0.11475831\n",
      "Iteration 560, loss = 0.11401767\n",
      "Iteration 561, loss = 0.11337160\n",
      "Iteration 562, loss = 0.11333495\n",
      "Iteration 563, loss = 0.11304530\n",
      "Iteration 564, loss = 0.11240766\n",
      "Iteration 565, loss = 0.11263281\n",
      "Iteration 566, loss = 0.11197490\n",
      "Iteration 567, loss = 0.11225346\n",
      "Iteration 568, loss = 0.11190785\n",
      "Iteration 569, loss = 0.11188916\n",
      "Iteration 570, loss = 0.11124745\n",
      "Iteration 571, loss = 0.11098255\n",
      "Iteration 572, loss = 0.11127442\n",
      "Iteration 573, loss = 0.11094023\n",
      "Iteration 574, loss = 0.11017222\n",
      "Iteration 575, loss = 0.10989800\n",
      "Iteration 576, loss = 0.11009154\n",
      "Iteration 577, loss = 0.10927174\n",
      "Iteration 578, loss = 0.10880257\n",
      "Iteration 579, loss = 0.10880907\n",
      "Iteration 580, loss = 0.10887403\n",
      "Iteration 581, loss = 0.10830686\n",
      "Iteration 582, loss = 0.10773114\n",
      "Iteration 583, loss = 0.10803961\n",
      "Iteration 584, loss = 0.10728598\n",
      "Iteration 585, loss = 0.10744494\n",
      "Iteration 586, loss = 0.10743593\n",
      "Iteration 587, loss = 0.10676852\n",
      "Iteration 588, loss = 0.10625440\n",
      "Iteration 589, loss = 0.10624589\n",
      "Iteration 590, loss = 0.10555744\n",
      "Iteration 591, loss = 0.10525270\n",
      "Iteration 592, loss = 0.10510235\n",
      "Iteration 593, loss = 0.10517935\n",
      "Iteration 594, loss = 0.10451414\n",
      "Iteration 595, loss = 0.10445461\n",
      "Iteration 596, loss = 0.10374539\n",
      "Iteration 597, loss = 0.10400129\n",
      "Iteration 598, loss = 0.10371716\n",
      "Iteration 599, loss = 0.10345434\n",
      "Iteration 600, loss = 0.10327879\n",
      "Iteration 601, loss = 0.10287935\n",
      "Iteration 602, loss = 0.10324321\n",
      "Iteration 603, loss = 0.10248678\n",
      "Iteration 604, loss = 0.10253291\n",
      "Iteration 605, loss = 0.10210065\n",
      "Iteration 606, loss = 0.10198145\n",
      "Iteration 607, loss = 0.10113410\n",
      "Iteration 608, loss = 0.10084835\n",
      "Iteration 609, loss = 0.10047043\n",
      "Iteration 610, loss = 0.10114793\n",
      "Iteration 611, loss = 0.10016384\n",
      "Iteration 612, loss = 0.09976906\n",
      "Iteration 613, loss = 0.10003946\n",
      "Iteration 614, loss = 0.09997180\n",
      "Iteration 615, loss = 0.09959158\n",
      "Iteration 616, loss = 0.09911437\n",
      "Iteration 617, loss = 0.09866707\n",
      "Iteration 618, loss = 0.09823296\n",
      "Iteration 619, loss = 0.09822920\n",
      "Iteration 620, loss = 0.09790323\n",
      "Iteration 621, loss = 0.09775002\n",
      "Iteration 622, loss = 0.09739489\n",
      "Iteration 623, loss = 0.09739776\n",
      "Iteration 624, loss = 0.09697737\n",
      "Iteration 625, loss = 0.09673822\n",
      "Iteration 626, loss = 0.09684053\n",
      "Iteration 627, loss = 0.09615433\n",
      "Iteration 628, loss = 0.09598148\n",
      "Iteration 629, loss = 0.09589238\n",
      "Iteration 630, loss = 0.09560965\n",
      "Iteration 631, loss = 0.09548336\n",
      "Iteration 632, loss = 0.09539079\n",
      "Iteration 633, loss = 0.09513708\n",
      "Iteration 634, loss = 0.09492279\n",
      "Iteration 635, loss = 0.09426340\n",
      "Iteration 636, loss = 0.09398220\n",
      "Iteration 637, loss = 0.09421901\n",
      "Iteration 638, loss = 0.09426665\n",
      "Iteration 639, loss = 0.09320300\n",
      "Iteration 640, loss = 0.09356363\n",
      "Iteration 641, loss = 0.09332197\n",
      "Iteration 642, loss = 0.09279191\n",
      "Iteration 643, loss = 0.09226411\n",
      "Iteration 644, loss = 0.09300394\n",
      "Iteration 645, loss = 0.09229729\n",
      "Iteration 646, loss = 0.09247416\n",
      "Iteration 647, loss = 0.09188622\n",
      "Iteration 648, loss = 0.09156724\n",
      "Iteration 649, loss = 0.09126242\n",
      "Iteration 650, loss = 0.09157052\n",
      "Iteration 651, loss = 0.09070301\n",
      "Iteration 652, loss = 0.09065585\n",
      "Iteration 653, loss = 0.09039805\n",
      "Iteration 654, loss = 0.08990490\n",
      "Iteration 655, loss = 0.09020672\n",
      "Iteration 656, loss = 0.08956070\n",
      "Iteration 657, loss = 0.08988553\n",
      "Iteration 658, loss = 0.08892024\n",
      "Iteration 659, loss = 0.08929425\n",
      "Iteration 660, loss = 0.08865327\n",
      "Iteration 661, loss = 0.08819265\n",
      "Iteration 662, loss = 0.08798699\n",
      "Iteration 663, loss = 0.08845497\n",
      "Iteration 664, loss = 0.08759809\n",
      "Iteration 665, loss = 0.08750523\n",
      "Iteration 666, loss = 0.08766166\n",
      "Iteration 667, loss = 0.08734067\n",
      "Iteration 668, loss = 0.08682421\n",
      "Iteration 669, loss = 0.08671843\n",
      "Iteration 670, loss = 0.08605530\n",
      "Iteration 671, loss = 0.08621346\n",
      "Iteration 672, loss = 0.08557759\n",
      "Iteration 673, loss = 0.08622446\n",
      "Iteration 674, loss = 0.08579480\n",
      "Iteration 675, loss = 0.08521186\n",
      "Iteration 676, loss = 0.08494355\n",
      "Iteration 677, loss = 0.08518656\n",
      "Iteration 678, loss = 0.08473419\n",
      "Iteration 679, loss = 0.08444663\n",
      "Iteration 680, loss = 0.08416342\n",
      "Iteration 681, loss = 0.08414731\n",
      "Iteration 682, loss = 0.08433575\n",
      "Iteration 683, loss = 0.08357153\n",
      "Iteration 684, loss = 0.08356527\n",
      "Iteration 685, loss = 0.08369585\n",
      "Iteration 686, loss = 0.08299956\n",
      "Iteration 687, loss = 0.08283258\n",
      "Iteration 688, loss = 0.08288862\n",
      "Iteration 689, loss = 0.08243776\n",
      "Iteration 690, loss = 0.08206606\n",
      "Iteration 691, loss = 0.08215013\n",
      "Iteration 692, loss = 0.08228368\n",
      "Iteration 693, loss = 0.08167144\n",
      "Iteration 694, loss = 0.08176018\n",
      "Iteration 695, loss = 0.08162749\n",
      "Iteration 696, loss = 0.08123865\n",
      "Iteration 697, loss = 0.08121516\n",
      "Iteration 698, loss = 0.08045191\n",
      "Iteration 699, loss = 0.08019680\n",
      "Iteration 700, loss = 0.08034790\n",
      "Iteration 701, loss = 0.08000601\n",
      "Iteration 702, loss = 0.07993082\n",
      "Iteration 703, loss = 0.07990996\n",
      "Iteration 704, loss = 0.07965938\n",
      "Iteration 705, loss = 0.07941648\n",
      "Iteration 706, loss = 0.07904317\n",
      "Iteration 707, loss = 0.07880448\n",
      "Iteration 708, loss = 0.07908953\n",
      "Iteration 709, loss = 0.07857099\n",
      "Iteration 710, loss = 0.07840268\n",
      "Iteration 711, loss = 0.07806933\n",
      "Iteration 712, loss = 0.07852707\n",
      "Iteration 713, loss = 0.07762496\n",
      "Iteration 714, loss = 0.07827862\n",
      "Iteration 715, loss = 0.07751995\n",
      "Iteration 716, loss = 0.07745869\n",
      "Iteration 717, loss = 0.07694601\n",
      "Iteration 718, loss = 0.07647341\n",
      "Iteration 719, loss = 0.07654561\n",
      "Iteration 720, loss = 0.07663816\n",
      "Iteration 721, loss = 0.07610041\n",
      "Iteration 722, loss = 0.07625081\n",
      "Iteration 723, loss = 0.07583149\n",
      "Iteration 724, loss = 0.07555585\n",
      "Iteration 725, loss = 0.07554649\n",
      "Iteration 726, loss = 0.07518614\n",
      "Iteration 727, loss = 0.07501301\n",
      "Iteration 728, loss = 0.07493181\n",
      "Iteration 729, loss = 0.07513313\n",
      "Iteration 730, loss = 0.07458713\n",
      "Iteration 731, loss = 0.07521546\n",
      "Iteration 732, loss = 0.07453422\n",
      "Iteration 733, loss = 0.07419633\n",
      "Iteration 734, loss = 0.07369188\n",
      "Iteration 735, loss = 0.07371911\n",
      "Iteration 736, loss = 0.07365806\n",
      "Iteration 737, loss = 0.07340328\n",
      "Iteration 738, loss = 0.07320614\n",
      "Iteration 739, loss = 0.07276348\n",
      "Iteration 740, loss = 0.07274695\n",
      "Iteration 741, loss = 0.07244228\n",
      "Iteration 742, loss = 0.07253559\n",
      "Iteration 743, loss = 0.07251040\n",
      "Iteration 744, loss = 0.07187354\n",
      "Iteration 745, loss = 0.07151494\n",
      "Iteration 746, loss = 0.07174426\n",
      "Iteration 747, loss = 0.07145668\n",
      "Iteration 748, loss = 0.07147472\n",
      "Iteration 749, loss = 0.07130559\n",
      "Iteration 750, loss = 0.07101964\n",
      "Iteration 751, loss = 0.07078879\n",
      "Iteration 752, loss = 0.07053017\n",
      "Iteration 753, loss = 0.07099634\n",
      "Iteration 754, loss = 0.07045765\n",
      "Iteration 755, loss = 0.07013270\n",
      "Iteration 756, loss = 0.06990736\n",
      "Iteration 757, loss = 0.06957552\n",
      "Iteration 758, loss = 0.06981344\n",
      "Iteration 759, loss = 0.07046668\n",
      "Iteration 760, loss = 0.06965108\n",
      "Iteration 761, loss = 0.06911195\n",
      "Iteration 762, loss = 0.06900295\n",
      "Iteration 763, loss = 0.06893916\n",
      "Iteration 764, loss = 0.06845965\n",
      "Iteration 765, loss = 0.06822052\n",
      "Iteration 766, loss = 0.06830377\n",
      "Iteration 767, loss = 0.06818631\n",
      "Iteration 768, loss = 0.06795179\n",
      "Iteration 769, loss = 0.06780859\n",
      "Iteration 770, loss = 0.06795571\n",
      "Iteration 771, loss = 0.06728740\n",
      "Iteration 772, loss = 0.06710209\n",
      "Iteration 773, loss = 0.06706623\n",
      "Iteration 774, loss = 0.06663764\n",
      "Iteration 775, loss = 0.06654902\n",
      "Iteration 776, loss = 0.06667381\n",
      "Iteration 777, loss = 0.06647660\n",
      "Iteration 778, loss = 0.06599823\n",
      "Iteration 779, loss = 0.06638860\n",
      "Iteration 780, loss = 0.06602141\n",
      "Iteration 781, loss = 0.06535404\n",
      "Iteration 782, loss = 0.06530610\n",
      "Iteration 783, loss = 0.06567460\n",
      "Iteration 784, loss = 0.06539366\n",
      "Iteration 785, loss = 0.06519674\n",
      "Iteration 786, loss = 0.06487798\n",
      "Iteration 787, loss = 0.06461856\n",
      "Iteration 788, loss = 0.06506858\n",
      "Iteration 789, loss = 0.06435443\n",
      "Iteration 790, loss = 0.06417621\n",
      "Iteration 791, loss = 0.06392308\n",
      "Iteration 792, loss = 0.06425815\n",
      "Iteration 793, loss = 0.06403364\n",
      "Iteration 794, loss = 0.06374293\n",
      "Iteration 795, loss = 0.06355385\n",
      "Iteration 796, loss = 0.06342788\n",
      "Iteration 797, loss = 0.06319592\n",
      "Iteration 798, loss = 0.06303733\n",
      "Iteration 799, loss = 0.06284373\n",
      "Iteration 800, loss = 0.06265255\n",
      "Iteration 801, loss = 0.06252869\n",
      "Iteration 802, loss = 0.06234551\n",
      "Iteration 803, loss = 0.06243040\n",
      "Iteration 804, loss = 0.06239210\n",
      "Iteration 805, loss = 0.06216485\n",
      "Iteration 806, loss = 0.06213174\n",
      "Iteration 807, loss = 0.06165975\n",
      "Iteration 808, loss = 0.06183220\n",
      "Iteration 809, loss = 0.06166622\n",
      "Iteration 810, loss = 0.06117060\n",
      "Iteration 811, loss = 0.06087517\n",
      "Iteration 812, loss = 0.06128312\n",
      "Iteration 813, loss = 0.06072260\n",
      "Iteration 814, loss = 0.06053993\n",
      "Iteration 815, loss = 0.06091385\n",
      "Iteration 816, loss = 0.06091749\n",
      "Iteration 817, loss = 0.06068994\n",
      "Iteration 818, loss = 0.06036128\n",
      "Iteration 819, loss = 0.05986896\n",
      "Iteration 820, loss = 0.05960883\n",
      "Iteration 821, loss = 0.05952362\n",
      "Iteration 822, loss = 0.05952669\n",
      "Iteration 823, loss = 0.05909910\n",
      "Iteration 824, loss = 0.05890787\n",
      "Iteration 825, loss = 0.05881652\n",
      "Iteration 826, loss = 0.05890252\n",
      "Iteration 827, loss = 0.05884385\n",
      "Iteration 828, loss = 0.05885279\n",
      "Iteration 829, loss = 0.05833450\n",
      "Iteration 830, loss = 0.05844979\n",
      "Iteration 831, loss = 0.05835191\n",
      "Iteration 832, loss = 0.05785266\n",
      "Iteration 833, loss = 0.05777238\n",
      "Iteration 834, loss = 0.05816745\n",
      "Iteration 835, loss = 0.05785462\n",
      "Iteration 836, loss = 0.05753622\n",
      "Iteration 837, loss = 0.05773228\n",
      "Iteration 838, loss = 0.05737337\n",
      "Iteration 839, loss = 0.05710522\n",
      "Iteration 840, loss = 0.05691051\n",
      "Iteration 841, loss = 0.05700299\n",
      "Iteration 842, loss = 0.05648984\n",
      "Iteration 843, loss = 0.05624128\n",
      "Iteration 844, loss = 0.05614710\n",
      "Iteration 845, loss = 0.05621183\n",
      "Iteration 846, loss = 0.05596485\n",
      "Iteration 847, loss = 0.05562067\n",
      "Iteration 848, loss = 0.05558654\n",
      "Iteration 849, loss = 0.05612792\n",
      "Iteration 850, loss = 0.05527476\n",
      "Iteration 851, loss = 0.05525222\n",
      "Iteration 852, loss = 0.05529779\n",
      "Iteration 853, loss = 0.05495381\n",
      "Iteration 854, loss = 0.05486597\n",
      "Iteration 855, loss = 0.05511949\n",
      "Iteration 856, loss = 0.05475253\n",
      "Iteration 857, loss = 0.05430937\n",
      "Iteration 858, loss = 0.05409972\n",
      "Iteration 859, loss = 0.05452559\n",
      "Iteration 860, loss = 0.05410041\n",
      "Iteration 861, loss = 0.05399824\n",
      "Iteration 862, loss = 0.05411309\n",
      "Iteration 863, loss = 0.05376785\n",
      "Iteration 864, loss = 0.05355435\n",
      "Iteration 865, loss = 0.05343969\n",
      "Iteration 866, loss = 0.05383312\n",
      "Iteration 867, loss = 0.05335587\n",
      "Iteration 868, loss = 0.05295853\n",
      "Iteration 869, loss = 0.05375311\n",
      "Iteration 870, loss = 0.05323374\n",
      "Iteration 871, loss = 0.05319755\n",
      "Iteration 872, loss = 0.05270932\n",
      "Iteration 873, loss = 0.05258689\n",
      "Iteration 874, loss = 0.05286073\n",
      "Iteration 875, loss = 0.05224955\n",
      "Iteration 876, loss = 0.05237905\n",
      "Iteration 877, loss = 0.05194703\n",
      "Iteration 878, loss = 0.05201054\n",
      "Iteration 879, loss = 0.05201765\n",
      "Iteration 880, loss = 0.05151696\n",
      "Iteration 881, loss = 0.05162945\n",
      "Iteration 882, loss = 0.05139688\n",
      "Iteration 883, loss = 0.05107959\n",
      "Iteration 884, loss = 0.05151874\n",
      "Iteration 885, loss = 0.05151754\n",
      "Iteration 886, loss = 0.05083193\n",
      "Iteration 887, loss = 0.05043091\n",
      "Iteration 888, loss = 0.05044412\n",
      "Iteration 889, loss = 0.05065200\n",
      "Iteration 890, loss = 0.05010643\n",
      "Iteration 891, loss = 0.05057596\n",
      "Iteration 892, loss = 0.05002789\n",
      "Iteration 893, loss = 0.04990938\n",
      "Iteration 894, loss = 0.04962648\n",
      "Iteration 895, loss = 0.04982076\n",
      "Iteration 896, loss = 0.04952202\n",
      "Iteration 897, loss = 0.04947845\n",
      "Iteration 898, loss = 0.04910375\n",
      "Iteration 899, loss = 0.04888698\n",
      "Iteration 900, loss = 0.04920341\n",
      "Iteration 901, loss = 0.04898809\n",
      "Iteration 902, loss = 0.04898729\n",
      "Iteration 903, loss = 0.04873464\n",
      "Iteration 904, loss = 0.04853555\n",
      "Iteration 905, loss = 0.04830843\n",
      "Iteration 906, loss = 0.04829020\n",
      "Iteration 907, loss = 0.04835655\n",
      "Iteration 908, loss = 0.04797630\n",
      "Iteration 909, loss = 0.04796317\n",
      "Iteration 910, loss = 0.04776140\n",
      "Iteration 911, loss = 0.04812250\n",
      "Iteration 912, loss = 0.04770776\n",
      "Iteration 913, loss = 0.04803813\n",
      "Iteration 914, loss = 0.04752745\n",
      "Iteration 915, loss = 0.04756699\n",
      "Iteration 916, loss = 0.04713720\n",
      "Iteration 917, loss = 0.04714227\n",
      "Iteration 918, loss = 0.04734483\n",
      "Iteration 919, loss = 0.04695573\n",
      "Iteration 920, loss = 0.04681465\n",
      "Iteration 921, loss = 0.04694260\n",
      "Iteration 922, loss = 0.04661242\n",
      "Iteration 923, loss = 0.04650943\n",
      "Iteration 924, loss = 0.04663966\n",
      "Iteration 925, loss = 0.04629231\n",
      "Iteration 926, loss = 0.04633281\n",
      "Iteration 927, loss = 0.04628954\n",
      "Iteration 928, loss = 0.04626451\n",
      "Iteration 929, loss = 0.04591387\n",
      "Iteration 930, loss = 0.04578118\n",
      "Iteration 931, loss = 0.04551516\n",
      "Iteration 932, loss = 0.04552198\n",
      "Iteration 933, loss = 0.04569261\n",
      "Iteration 934, loss = 0.04530779\n",
      "Iteration 935, loss = 0.04544650\n",
      "Iteration 936, loss = 0.04525751\n",
      "Iteration 937, loss = 0.04514088\n",
      "Iteration 938, loss = 0.04484882\n",
      "Iteration 939, loss = 0.04450768\n",
      "Iteration 940, loss = 0.04448052\n",
      "Iteration 941, loss = 0.04452428\n",
      "Iteration 942, loss = 0.04437782\n",
      "Iteration 943, loss = 0.04433664\n",
      "Iteration 944, loss = 0.04406904\n",
      "Iteration 945, loss = 0.04396881\n",
      "Iteration 946, loss = 0.04403500\n",
      "Iteration 947, loss = 0.04385353\n",
      "Iteration 948, loss = 0.04355864\n",
      "Iteration 949, loss = 0.04351606\n",
      "Iteration 950, loss = 0.04348360\n",
      "Iteration 951, loss = 0.04334617\n",
      "Iteration 952, loss = 0.04322796\n",
      "Iteration 953, loss = 0.04324343\n",
      "Iteration 954, loss = 0.04313325\n",
      "Iteration 955, loss = 0.04309524\n",
      "Iteration 956, loss = 0.04278047\n",
      "Iteration 957, loss = 0.04344069\n",
      "Iteration 958, loss = 0.04283959\n",
      "Iteration 959, loss = 0.04286368\n",
      "Iteration 960, loss = 0.04284064\n",
      "Iteration 961, loss = 0.04246902\n",
      "Iteration 962, loss = 0.04251486\n",
      "Iteration 963, loss = 0.04221274\n",
      "Iteration 964, loss = 0.04209298\n",
      "Iteration 965, loss = 0.04199696\n",
      "Iteration 966, loss = 0.04187190\n",
      "Iteration 967, loss = 0.04166425\n",
      "Iteration 968, loss = 0.04160051\n",
      "Iteration 969, loss = 0.04151708\n",
      "Iteration 970, loss = 0.04137020\n",
      "Iteration 971, loss = 0.04149821\n",
      "Iteration 972, loss = 0.04115489\n",
      "Iteration 973, loss = 0.04095823\n",
      "Iteration 974, loss = 0.04097405\n",
      "Iteration 975, loss = 0.04111144\n",
      "Iteration 976, loss = 0.04074072\n",
      "Iteration 977, loss = 0.04105133\n",
      "Iteration 978, loss = 0.04114980\n",
      "Iteration 979, loss = 0.04051084\n",
      "Iteration 980, loss = 0.04051783\n",
      "Iteration 981, loss = 0.04024798\n",
      "Iteration 982, loss = 0.04039359\n",
      "Iteration 983, loss = 0.04019492\n",
      "Iteration 984, loss = 0.04020497\n",
      "Iteration 985, loss = 0.03993313\n",
      "Iteration 986, loss = 0.04000860\n",
      "Iteration 987, loss = 0.03997335\n",
      "Iteration 988, loss = 0.03987750\n",
      "Iteration 989, loss = 0.03956413\n",
      "Iteration 990, loss = 0.03946202\n",
      "Iteration 991, loss = 0.03933481\n",
      "Iteration 992, loss = 0.03934106\n",
      "Iteration 993, loss = 0.03911124\n",
      "Iteration 994, loss = 0.03916002\n",
      "Iteration 995, loss = 0.03914072\n",
      "Iteration 996, loss = 0.03905362\n",
      "Iteration 997, loss = 0.03882305\n",
      "Iteration 998, loss = 0.03871329\n",
      "Iteration 999, loss = 0.03864823\n",
      "Iteration 1000, loss = 0.03862242\n",
      "Iteration 1001, loss = 0.03861746\n",
      "Iteration 1002, loss = 0.03844018\n",
      "Iteration 1003, loss = 0.03835069\n",
      "Iteration 1004, loss = 0.03858528\n",
      "Iteration 1005, loss = 0.03827134\n",
      "Iteration 1006, loss = 0.03784814\n",
      "Iteration 1007, loss = 0.03794424\n",
      "Iteration 1008, loss = 0.03798380\n",
      "Iteration 1009, loss = 0.03759615\n",
      "Iteration 1010, loss = 0.03762793\n",
      "Iteration 1011, loss = 0.03763398\n",
      "Iteration 1012, loss = 0.03751452\n",
      "Iteration 1013, loss = 0.03734577\n",
      "Iteration 1014, loss = 0.03747818\n",
      "Iteration 1015, loss = 0.03737595\n",
      "Iteration 1016, loss = 0.03717102\n",
      "Iteration 1017, loss = 0.03715486\n",
      "Iteration 1018, loss = 0.03715875\n",
      "Iteration 1019, loss = 0.03687876\n",
      "Iteration 1020, loss = 0.03689411\n",
      "Iteration 1021, loss = 0.03665078\n",
      "Iteration 1022, loss = 0.03674005\n",
      "Iteration 1023, loss = 0.03684107\n",
      "Iteration 1024, loss = 0.03641460\n",
      "Iteration 1025, loss = 0.03623315\n",
      "Iteration 1026, loss = 0.03615349\n",
      "Iteration 1027, loss = 0.03598689\n",
      "Iteration 1028, loss = 0.03599341\n",
      "Iteration 1029, loss = 0.03588099\n",
      "Iteration 1030, loss = 0.03577499\n",
      "Iteration 1031, loss = 0.03574977\n",
      "Iteration 1032, loss = 0.03576421\n",
      "Iteration 1033, loss = 0.03566891\n",
      "Iteration 1034, loss = 0.03548851\n",
      "Iteration 1035, loss = 0.03542009\n",
      "Iteration 1036, loss = 0.03528493\n",
      "Iteration 1037, loss = 0.03515939\n",
      "Iteration 1038, loss = 0.03529139\n",
      "Iteration 1039, loss = 0.03510685\n",
      "Iteration 1040, loss = 0.03516842\n",
      "Iteration 1041, loss = 0.03496380\n",
      "Iteration 1042, loss = 0.03492045\n",
      "Iteration 1043, loss = 0.03481707\n",
      "Iteration 1044, loss = 0.03464569\n",
      "Iteration 1045, loss = 0.03470619\n",
      "Iteration 1046, loss = 0.03450984\n",
      "Iteration 1047, loss = 0.03448857\n",
      "Iteration 1048, loss = 0.03434664\n",
      "Iteration 1049, loss = 0.03429090\n",
      "Iteration 1050, loss = 0.03446223\n",
      "Iteration 1051, loss = 0.03430442\n",
      "Iteration 1052, loss = 0.03397754\n",
      "Iteration 1053, loss = 0.03440123\n",
      "Iteration 1054, loss = 0.03414372\n",
      "Iteration 1055, loss = 0.03406728\n",
      "Iteration 1056, loss = 0.03385727\n",
      "Iteration 1057, loss = 0.03363129\n",
      "Iteration 1058, loss = 0.03366865\n",
      "Iteration 1059, loss = 0.03345859\n",
      "Iteration 1060, loss = 0.03365079\n",
      "Iteration 1061, loss = 0.03334015\n",
      "Iteration 1062, loss = 0.03319742\n",
      "Iteration 1063, loss = 0.03335647\n",
      "Iteration 1064, loss = 0.03315538\n",
      "Iteration 1065, loss = 0.03300905\n",
      "Iteration 1066, loss = 0.03298314\n",
      "Iteration 1067, loss = 0.03293167\n",
      "Iteration 1068, loss = 0.03274187\n",
      "Iteration 1069, loss = 0.03287037\n",
      "Iteration 1070, loss = 0.03274002\n",
      "Iteration 1071, loss = 0.03279239\n",
      "Iteration 1072, loss = 0.03244514\n",
      "Iteration 1073, loss = 0.03256949\n",
      "Iteration 1074, loss = 0.03264554\n",
      "Iteration 1075, loss = 0.03243260\n",
      "Iteration 1076, loss = 0.03240133\n",
      "Iteration 1077, loss = 0.03211551\n",
      "Iteration 1078, loss = 0.03206429\n",
      "Iteration 1079, loss = 0.03188958\n",
      "Iteration 1080, loss = 0.03215032\n",
      "Iteration 1081, loss = 0.03163186\n",
      "Iteration 1082, loss = 0.03169342\n",
      "Iteration 1083, loss = 0.03166469\n",
      "Iteration 1084, loss = 0.03152858\n",
      "Iteration 1085, loss = 0.03137008\n",
      "Iteration 1086, loss = 0.03138154\n",
      "Iteration 1087, loss = 0.03141848\n",
      "Iteration 1088, loss = 0.03137268\n",
      "Iteration 1089, loss = 0.03116323\n",
      "Iteration 1090, loss = 0.03126422\n",
      "Iteration 1091, loss = 0.03097396\n",
      "Iteration 1092, loss = 0.03083278\n",
      "Iteration 1093, loss = 0.03094809\n",
      "Iteration 1094, loss = 0.03088882\n",
      "Iteration 1095, loss = 0.03053018\n",
      "Iteration 1096, loss = 0.03061937\n",
      "Iteration 1097, loss = 0.03066086\n",
      "Iteration 1098, loss = 0.03052512\n",
      "Iteration 1099, loss = 0.03053463\n",
      "Iteration 1100, loss = 0.03040176\n",
      "Iteration 1101, loss = 0.03025279\n",
      "Iteration 1102, loss = 0.03006309\n",
      "Iteration 1103, loss = 0.03033798\n",
      "Iteration 1104, loss = 0.02993275\n",
      "Iteration 1105, loss = 0.02983998\n",
      "Iteration 1106, loss = 0.02993479\n",
      "Iteration 1107, loss = 0.02969771\n",
      "Iteration 1108, loss = 0.02962467\n",
      "Iteration 1109, loss = 0.02947626\n",
      "Iteration 1110, loss = 0.02938618\n",
      "Iteration 1111, loss = 0.02925249\n",
      "Iteration 1112, loss = 0.02917516\n",
      "Iteration 1113, loss = 0.02934826\n",
      "Iteration 1114, loss = 0.02898761\n",
      "Iteration 1115, loss = 0.02924057\n",
      "Iteration 1116, loss = 0.02902297\n",
      "Iteration 1117, loss = 0.02879381\n",
      "Iteration 1118, loss = 0.02889997\n",
      "Iteration 1119, loss = 0.02860575\n",
      "Iteration 1120, loss = 0.02845765\n",
      "Iteration 1121, loss = 0.02852740\n",
      "Iteration 1122, loss = 0.02841589\n",
      "Iteration 1123, loss = 0.02862964\n",
      "Iteration 1124, loss = 0.02825533\n",
      "Iteration 1125, loss = 0.02832605\n",
      "Iteration 1126, loss = 0.02834712\n",
      "Iteration 1127, loss = 0.02796710\n",
      "Iteration 1128, loss = 0.02790719\n",
      "Iteration 1129, loss = 0.02793757\n",
      "Iteration 1130, loss = 0.02766259\n",
      "Iteration 1131, loss = 0.02761355\n",
      "Iteration 1132, loss = 0.02783212\n",
      "Iteration 1133, loss = 0.02755153\n",
      "Iteration 1134, loss = 0.02748636\n",
      "Iteration 1135, loss = 0.02731511\n",
      "Iteration 1136, loss = 0.02723975\n",
      "Iteration 1137, loss = 0.02714662\n",
      "Iteration 1138, loss = 0.02707411\n",
      "Iteration 1139, loss = 0.02708598\n",
      "Iteration 1140, loss = 0.02705042\n",
      "Iteration 1141, loss = 0.02692521\n",
      "Iteration 1142, loss = 0.02686623\n",
      "Iteration 1143, loss = 0.02677604\n",
      "Iteration 1144, loss = 0.02672245\n",
      "Iteration 1145, loss = 0.02654683\n",
      "Iteration 1146, loss = 0.02656875\n",
      "Iteration 1147, loss = 0.02652199\n",
      "Iteration 1148, loss = 0.02632124\n",
      "Iteration 1149, loss = 0.02624765\n",
      "Iteration 1150, loss = 0.02614272\n",
      "Iteration 1151, loss = 0.02600024\n",
      "Iteration 1152, loss = 0.02621473\n",
      "Iteration 1153, loss = 0.02603254\n",
      "Iteration 1154, loss = 0.02604297\n",
      "Iteration 1155, loss = 0.02593964\n",
      "Iteration 1156, loss = 0.02565951\n",
      "Iteration 1157, loss = 0.02572989\n",
      "Iteration 1158, loss = 0.02597344\n",
      "Iteration 1159, loss = 0.02551300\n",
      "Iteration 1160, loss = 0.02573740\n",
      "Iteration 1161, loss = 0.02547956\n",
      "Iteration 1162, loss = 0.02526728\n",
      "Iteration 1163, loss = 0.02530617\n",
      "Iteration 1164, loss = 0.02524864\n",
      "Iteration 1165, loss = 0.02506764\n",
      "Iteration 1166, loss = 0.02512715\n",
      "Iteration 1167, loss = 0.02514862\n",
      "Iteration 1168, loss = 0.02490179\n",
      "Iteration 1169, loss = 0.02478251\n",
      "Iteration 1170, loss = 0.02483348\n",
      "Iteration 1171, loss = 0.02461142\n",
      "Iteration 1172, loss = 0.02455581\n",
      "Iteration 1173, loss = 0.02465289\n",
      "Iteration 1174, loss = 0.02449825\n",
      "Iteration 1175, loss = 0.02426143\n",
      "Iteration 1176, loss = 0.02432493\n",
      "Iteration 1177, loss = 0.02424510\n",
      "Iteration 1178, loss = 0.02415690\n",
      "Iteration 1179, loss = 0.02407318\n",
      "Iteration 1180, loss = 0.02416139\n",
      "Iteration 1181, loss = 0.02395586\n",
      "Iteration 1182, loss = 0.02388679\n",
      "Iteration 1183, loss = 0.02380924\n",
      "Iteration 1184, loss = 0.02372287\n",
      "Iteration 1185, loss = 0.02364879\n",
      "Iteration 1186, loss = 0.02370276\n",
      "Iteration 1187, loss = 0.02376599\n",
      "Iteration 1188, loss = 0.02350255\n",
      "Iteration 1189, loss = 0.02349642\n",
      "Iteration 1190, loss = 0.02339186\n",
      "Iteration 1191, loss = 0.02325921\n",
      "Iteration 1192, loss = 0.02325968\n",
      "Iteration 1193, loss = 0.02333082\n",
      "Iteration 1194, loss = 0.02336503\n",
      "Iteration 1195, loss = 0.02325147\n",
      "Iteration 1196, loss = 0.02312720\n",
      "Iteration 1197, loss = 0.02289420\n",
      "Iteration 1198, loss = 0.02284338\n",
      "Iteration 1199, loss = 0.02291149\n",
      "Iteration 1200, loss = 0.02282396\n",
      "Iteration 1201, loss = 0.02258293\n",
      "Iteration 1202, loss = 0.02264574\n",
      "Iteration 1203, loss = 0.02247888\n",
      "Iteration 1204, loss = 0.02248175\n",
      "Iteration 1205, loss = 0.02250019\n",
      "Iteration 1206, loss = 0.02225395\n",
      "Iteration 1207, loss = 0.02233353\n",
      "Iteration 1208, loss = 0.02233770\n",
      "Iteration 1209, loss = 0.02208360\n",
      "Iteration 1210, loss = 0.02212093\n",
      "Iteration 1211, loss = 0.02204510\n",
      "Iteration 1212, loss = 0.02197579\n",
      "Iteration 1213, loss = 0.02197381\n",
      "Iteration 1214, loss = 0.02184661\n",
      "Iteration 1215, loss = 0.02171513\n",
      "Iteration 1216, loss = 0.02173694\n",
      "Iteration 1217, loss = 0.02163448\n",
      "Iteration 1218, loss = 0.02159334\n",
      "Iteration 1219, loss = 0.02173089\n",
      "Iteration 1220, loss = 0.02148675\n",
      "Iteration 1221, loss = 0.02150501\n",
      "Iteration 1222, loss = 0.02142408\n",
      "Iteration 1223, loss = 0.02139958\n",
      "Iteration 1224, loss = 0.02131972\n",
      "Iteration 1225, loss = 0.02148753\n",
      "Iteration 1226, loss = 0.02134981\n",
      "Iteration 1227, loss = 0.02112937\n",
      "Iteration 1228, loss = 0.02106627\n",
      "Iteration 1229, loss = 0.02118207\n",
      "Iteration 1230, loss = 0.02110574\n",
      "Iteration 1231, loss = 0.02082451\n",
      "Iteration 1232, loss = 0.02092614\n",
      "Iteration 1233, loss = 0.02077682\n",
      "Iteration 1234, loss = 0.02062285\n",
      "Iteration 1235, loss = 0.02081110\n",
      "Iteration 1236, loss = 0.02058648\n",
      "Iteration 1237, loss = 0.02048901\n",
      "Iteration 1238, loss = 0.02046428\n",
      "Iteration 1239, loss = 0.02039033\n",
      "Iteration 1240, loss = 0.02043160\n",
      "Iteration 1241, loss = 0.02038287\n",
      "Iteration 1242, loss = 0.02025609\n",
      "Iteration 1243, loss = 0.02031163\n",
      "Iteration 1244, loss = 0.02023531\n",
      "Iteration 1245, loss = 0.02007500\n",
      "Iteration 1246, loss = 0.02003534\n",
      "Iteration 1247, loss = 0.02008070\n",
      "Iteration 1248, loss = 0.02003643\n",
      "Iteration 1249, loss = 0.01978971\n",
      "Iteration 1250, loss = 0.01998190\n",
      "Iteration 1251, loss = 0.01976458\n",
      "Iteration 1252, loss = 0.01979617\n",
      "Iteration 1253, loss = 0.01966897\n",
      "Iteration 1254, loss = 0.01958329\n",
      "Iteration 1255, loss = 0.01953257\n",
      "Iteration 1256, loss = 0.01957419\n",
      "Iteration 1257, loss = 0.01947453\n",
      "Iteration 1258, loss = 0.01938616\n",
      "Iteration 1259, loss = 0.01944082\n",
      "Iteration 1260, loss = 0.01949826\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(60,), max_iter=3000, verbose=True)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train_mlp,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "59024d8c-5773-451d-81cc-a4c4c10136c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 7, 9, 0, 7, 5, 0, 3, 6, 4, 1, 4, 7, 4, 5, 2, 5, 9, 9, 7, 3, 5,\n",
       "       6, 8, 3, 3, 3, 0, 4, 2, 6, 5, 5, 1, 4, 2, 6, 7, 5, 4, 0, 5, 9, 1,\n",
       "       1, 4, 9, 8, 5, 0, 1, 6, 5, 0, 1, 7, 7, 1, 6, 4, 0, 8, 7, 7, 5, 7,\n",
       "       8, 0, 4, 9, 5, 3, 1, 6, 2, 2, 9, 1, 0, 4, 8, 2, 1, 1, 2, 2, 6, 8,\n",
       "       5, 3, 2, 7, 3, 3, 9, 1, 1, 0, 0, 3], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.predict(X_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9e7253db-81db-4d20-a60f-cfc3fe325135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,mlp.predict(X_test_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc97f83-0469-4e35-9739-c8dd8861141b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
